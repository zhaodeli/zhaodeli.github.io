<html>

<head>
<title>Welcome to Deli Zhao's Homepage</title>
<style>
a{text-decoration:none;}
</style>
</head>

<body  leftmargin="50" rightmargin="0">

<table width="80%" border="0" bordercolor="#FFFFFF" id="table3">
<tr>
<td width="196" style="padding: 0">
<img border="0" src="zhaodeli2.jpg" width="217" height="217"></td>
<td width="941" style="padding: 0" height="0"><h2>Deli Zhao | 赵德丽</h2>
<p>Director on foundation models for computer vision<br> 
<br>
  Homepage: https://zhaodeli.github.io
<br><p></p>
E-mail: zhaodeli AT gmail.com</p>
Publication: <a href="https://scholar.google.com/citations?hl=en&user=7LhjCn0AAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a> | <a href="https://dblp.org/pid/77/1992.html">DBLP</a> </p>
</td>
</tr>
</table>

<hr>


<h2><font size="5">About</font></h2>
<ul>
Deli Zhao is the director on foundation models, leading the studies on large-scale visual models as well as their applications. Working with <a href="https://cn.linkedin.com/in/jrzhou">Dr. Jingren Zhou</a>, his team has released Composer 2.0 and VideoComposer 1.0, a member of Alibaba <a href="https://tongyi.aliyun.com">Tongyi</a> big models, for customizable and creative image generation and video synthesis, respectively. Before joining Alibaba, Deli worked at HTC and Xiaomi as the research director for six years. He also spent six wonderful years doing research in the Visual Computing Group of Microsoft Research Asia (MSRA) and in the Multimedia Lab (MMLab) at the Chinese University of Hong Kong (CUHK), working with <a href="http://www.ie.cuhk.edu.hk/people/xotang.shtml">Prof. Xiaoou Tang</a>. Deli has performed research on computer vision and machine learning for nearly two decades, mainly focusing on generative models, multi-modal learning, and large-scale pre-training. He has published leading papers pertaining to generative adversarial networks, diffusion models, foundation models, and fundamental principle of deep learning.
</ul>
<p></p>


<h2><font size="5">Research Work on Foundation Models</font></h2>
  <h2><font size="4"> Generative Models</font></h2>
<ul>
  <li><strong>Text-to-image:</strong> <a href="https://damo-vilab.github.io/composer-page/">Composer</a>, <a href="https://cones-page.github.io">Cones</a>,  <a href="https://damo-vilab.github.io/AnyDoor-Page/">AnyDoor</a> </li>
  <li><strong>Text-to-video:</strong> <a href="https://videocomposer.github.io">VideoComposer</a>, <a href="https://arxiv.org/abs/2303.08320">VideoFusion</a>,  FaceComposer </li>
  <li><strong>Diffusion model:</strong> <a href="https://arxiv.org/abs/2211.16032">Dimensionality-Varying Diffusion Process</a>, <a href="https://arxiv.org/abs/2306.11251">Eliminating Lipschitz Singularities</a> </li>
  <li><strong>Generative Adversarial Network:</strong> <a href="https://arxiv.org/abs/1906.08090v1">Latently Invertible Autoencoder (LIA)</a>, <a href="https://zhujiapeng.github.io/LowRankGAN/">LowRankGAN</a>,  <a href="https://zhujiapeng.github.io/resefa/">ReSeFa</a>,  <a href="https://zhujiapeng.github.io/linkgan/">LinkGAN</a>  </li>
</ul>
  <h2><font size="4"> Multi-modal Learning</font></h2>
  <ul>
  <li><strong>Image:</strong> <a href="https://github.com/JacobYuan7/RLIPv2">RLIP</a>, <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xie_RA-CLIP_Retrieval_Augmented_Contrastive_Language-Image_Pre-Training_CVPR_2023_paper.html">RA-CLIP</a>, <a href="https://proceedings.mlr.press/v202/zhao23l.html">RLEG</a> </li>
  <li><strong>Video:</strong> <a href="https://arxiv.org/abs/2307.02869">MomentDiff</a> </li>
   <li><strong>Unified framework:</strong>  <a href="https://arxiv.org/abs/2303.06911">ViM</a>,  <a href="https://arxiv.org/abs/2303.00690">U-Tuning</a> </li>
  </ul>
<p></p>

<h2><font size="5">Projects on AIGC </font></h2>
<ul>
<li><strong>Text-to-image:</strong> <a href=" https://wanxiang.aliyun.com "> 通义万相 </a>  </li>
<li><strong>Text-to-video:</strong> <a href="https://huggingface.co/damo-vilab/MS-Vid2Vid-XL"> Video generation </a>, <a href="https://huggingface.co/damo-vilab"> Open-source models on Hugging Face </a>, <a href="https://modelscope.cn/models/damo/Image-to-Video/summary"> Open-source models on ModelScope </a> </li>
</ul>

</body>

</html>
