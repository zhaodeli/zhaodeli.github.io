<html>

<head>
<title>Welcome to Deli Zhao's Homepage</title>
<style>
a{text-decoration:none;}
</style>
</head>

<body  leftmargin="50" rightmargin="0">

<table width="80%" border="0" bordercolor="#FFFFFF" id="table3">
<tr>
<td width="196" style="padding: 0">
<img border="0" src="zhaodeli2.jpg" width="217" height="217"></td>
<td width="941" style="padding: 0" height="0"><h2>Deli Zhao | 赵德丽</h2>
<p>Director on foundation models for computer vision<br> 
<br>
  <strong>Homepage</strong>: https://zhaodeli.github.io
<br><p></p>
<strong>E-mail</strong>: zhaodeli AT gmail.com</p>
<strong>Publication</strong>: <a href="https://scholar.google.com/citations?hl=en&user=7LhjCn0AAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a> | <a href="https://dblp.org/pid/77/1992.html">DBLP</a> </p>
</td>
</tr>
</table>

<hr>


<h2><font size="5">About</font></h2>
<ul>
Deli Zhao is the director on foundation models, leading the studies on large-scale visual models as well as their applications. Working with <a href="https://cn.linkedin.com/in/jrzhou">Dr. Jingren Zhou</a>, my team has released <font color="#CA2704"> <strong>Composer</strong> </font> 2.0 and <font color="#CA2704"><strong>VideoComposer</strong></font> 1.0, a member of <a href="https://tongyi.aliyun.com">Alibaba Tongyi</a> big models, for customizable and creative image generation and video synthesis, respectively. Before joining Alibaba, Deli worked at HTC and Xiaomi as the research director for six years. He also spent six wonderful years doing research in the Visual Computing Group of Microsoft Research Asia (MSRA) and in the Multimedia Lab (MMLab) at the Chinese University of Hong Kong (CUHK), working with <a href="http://www.ie.cuhk.edu.hk/people/xotang.shtml">Prof. Xiaoou Tang</a>. Deli has performed research on computer vision and machine learning for nearly two decades, mainly focusing on generative models, multi-modal learning, and large-scale pre-training. He has published leading papers pertaining to generative adversarial networks, diffusion models, foundation models, and fundamental principle of deep learning.
</ul>
<p></p>


<h2><font size="5">Research</font></h2>
  <h2><font size="4"> Generative Foundation Models</font></h2>
<ul>
  <li><strong>Text-to-image:</strong> <a href="https://damo-vilab.github.io/composer-page/"> <strong> Composer</strong></a>, <a href="https://cones-page.github.io">Cones</a>,  <a href="https://damo-vilab.github.io/AnyDoor-Page/">AnyDoor</a> </li>
  <li><strong>Text-to-video:</strong> <a href="https://videocomposer.github.io"><strong> VideoComposer</strong></a>, <a href="https://arxiv.org/abs/2303.08320">VideoFusion</a>,  FaceComposer </li>
  <li><strong>Diffusion model:</strong> <a href="https://arxiv.org/abs/2211.16032">Dimensionality-Varying Diffusion Process</a>, <a href="https://arxiv.org/abs/2306.11251">Eliminating Lipschitz Singularities</a> </li>
  <li><strong>Generative Adversarial Network:</strong> <a href="https://arxiv.org/abs/1906.08090v1">Latently Invertible Autoencoder (LIA)</a>, <a href="https://genforce.github.io/idinvert/">IDInvert</a>, <a href="https://zhujiapeng.github.io/LowRankGAN/">LowRankGAN</a>,  <a href="https://zhujiapeng.github.io/resefa/">ReSeFa</a>,  <a href="https://zhujiapeng.github.io/linkgan/">LinkGAN</a>  </li>
</ul>
  <h2><font size="4"> Multi-Modal Learning</font></h2>
  <ul>
  <li><strong>Image:</strong> <a href="https://github.com/JacobYuan7/RLIPv2">RLIP</a>, <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xie_RA-CLIP_Retrieval_Augmented_Contrastive_Language-Image_Pre-Training_CVPR_2023_paper.html">RA-CLIP</a>, <a href="https://proceedings.mlr.press/v202/zhao23l.html">RLEG</a> </li>
  <li><strong>Video:</strong> <a href="https://arxiv.org/abs/2307.02869">MomentDiff</a> </li>
   <li><strong>Unified framework:</strong>  <a href="https://arxiv.org/abs/2303.06911">ViM</a>,  <a href="https://arxiv.org/abs/2303.00690">U-Tuning</a> </li>
  </ul>
  <h2><font size="4"> Deep-Learning Theory</font></h2>
  <ul>
  <li><strong>Deep neural networks:</strong>  <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/d5cd70b708f726737e2ebace18c3f71b-Abstract-Conference.html">Rank Diminishing in DNNs</a> (cowork with Michael Jordan),  <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Feng_Neural_Dependencies_Emerging_From_Learning_Massive_Categories_CVPR_2023_paper.html">Neural Dependencies</a> (cowork with Michael Jordan) </li>
  <li><strong>Generative models:</strong>  <a href="https://proceedings.mlr.press/v139/feng21g.html">Noise injection in GANs</a>, <a href="https://proceedings.mlr.press/v139/feng21c.html">Uncertainty Principles of Encoding GANs</a> </li>
  
  </ul>
<p></p>

<h2><font size="5">Projects on AIGC </font></h2>
<ul>
<li><strong>Text-to-image:</strong> <a href=" https://wanxiang.aliyun.com "> <strong"> 通义万相 </strong> </a>  </li>
<li><strong>Text-to-video:</strong> <a href="https://huggingface.co/damo-vilab/MS-Vid2Vid-XL"> <strong> Video generation</strong></a>, <a href="https://huggingface.co/damo-vilab"> Open-source models on Hugging Face</a>, <a href="https://modelscope.cn/models/damo/Image-to-Video/summary"> Open-source models on ModelScope</a> </li>
</ul>

</body>

</html>
