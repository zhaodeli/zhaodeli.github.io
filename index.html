<html>

<head>
<title>Welcome to Deli Zhao's Homepage</title>
<style>
a{text-decoration:none;}
</style>
</head>

<body  leftmargin="50" rightmargin="0">

<table width="80%" border="0" bordercolor="#FFFFFF" id="table3">
<tr>
<td width="196" style="padding: 0">
<img border="0" src="zhaodeli2.jpg" width="217" height="217"></td>
<td width="941" style="padding: 0" height="0"><h2>Deli Zhao | 赵德丽</h2>
<p><font size="4">Director on foundation models for computer vision</font><br> 
 <font size="4">Consultant on AI</font>
<br><p></p>
<strong>E-mail</strong>: zhaodeli AT gmail.com</p>
<strong>Publication</strong>: <a href="https://scholar.google.com/citations?hl=en&user=7LhjCn0AAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a> | <a href="https://dblp.org/pid/77/1992.html">DBLP</a> </p>
</td>
</tr>
</table>

<hr>


<h2><font size="5">About Me</font></h2>
<ul>
Deli Zhao is the director on foundation models, leading the studies on large-scale visual models as well as their applications. Working with <a href="https://cn.linkedin.com/in/jrzhou">Dr. Jingren Zhou</a>, my team has released <font color="#C20505"> <strong>Composer</strong> </font> 2.0 and <font color="#C20505"><strong>VideoComposer</strong></font> 1.0, a member of <a href="https://tongyi.aliyun.com">Alibaba Tongyi</a> big models, for customizable and creative image generation and video synthesis, respectively. Before joining Alibaba, I worked at HTC and Xiaomi as the research director for six years. I also spent six wonderful years doing research in the Visual Computing Group of Microsoft Research Asia (MSRA) and in the Multimedia Lab (MMLab) at the Chinese University of Hong Kong (CUHK), working with <a href="http://www.ie.cuhk.edu.hk/people/xotang.shtml">Prof. Xiaoou Tang</a>. I have performed research on computer vision and machine learning for nearly two decades, now mainly focusing on generative models, multi-modal learning, and foundation models. I am also interested in the fundamental principle of deep learning (two theoretical papers coworking with <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/jordan.html">Prof. Michael Jordan</a>). I closely work with <a href="https://shenyujun.github.io">Dr. Yujun Shen</a> on generative models.
</ul>
<p></p>

<h2><font size="5">News</font></h2>
<ul>
  <li><strong>[09/2023]:</strong> Keynote on "<font color="#C20505"><strong>Towards On-Chain AI</strong></font>" at Shanghai Blockchain Week. </li>
  <li><strong>[07/2023]:</strong> 12 papers accepted by <a href="https://iccv2023.thecvf.com">ICCV</a>. The acceptance rate in my team is 70.6% (official 26.8%). </li>
  <li><strong>[06/2023]:</strong> We released <a href="https://videocomposer.github.io"><strong> VideoComposer</strong></a>, the Alibaba big model on video generation and editing. </li>
  <li><strong>[06/2023]:</strong> Keynote and round-table discussion on AIGC and design at <a href="https://udw.digitalexpo.com/pc/home">U Design Week</a>.  </li>
  <li><strong>[04/2023]:</strong> 3 papers accepted by <a href="https://icml.cc/Conferences/2023/Dates">ICML</a>. The acceptance rate in my team is 75% (official 27.9%). </li>
  <li><strong>[04/2023]:</strong> Talk on Composer at <a href="https://www.hku.hk">The University of Hong Kong</a>.  </li>
  <li><strong>[04/2023]:</strong> Talk on Composer at <a href="https://hkust.edu.hk">The Hong Kong University of Science and Technology</a>.  </li>
  <li><strong>[04/2023]:</strong> Keynote on <font color="#C20505"><strong>AIGC</strong></font> at <a href="https://www.web3festival.org/hongkong2023/agenda?lang=en-US"><strong>Hong Kong Web3 Festival</strong></a>.  </li>
  <li><strong>[02/2023]:</strong> 10 papers accepted by <a href="https://cvpr2023.thecvf.com/Conferences/2023">CVPR</a>. </li>
  <li><strong>[02/2023]:</strong> We released <a href="https://damo-vilab.github.io/composer-page/"> <strong> Composer</strong></a>, the Alibaba big model on image generation and editing. </li>
</ul>
  

<hr>
  
<h2><font size="5">Research</font></h2>
  <h2><font size="4"> Generative Foundation Models</font></h2>
<ul>
  <li><strong>Text-to-image:</strong> <a href="https://damo-vilab.github.io/composer-page/"> <strong> Composer</strong></a>, <a href="https://cones-page.github.io">Cones</a>,  <a href="https://damo-vilab.github.io/AnyDoor-Page/">AnyDoor</a> </li>
  <li><strong>Text-to-video:</strong> <a href="https://videocomposer.github.io"><strong> VideoComposer</strong></a>, <a href="https://arxiv.org/abs/2303.08320">VideoFusion</a>,  FaceComposer </li>
  <li><strong>Diffusion model:</strong> <a href="https://arxiv.org/abs/2211.16032">Dimensionality-Varying Diffusion Process</a>, <a href="https://arxiv.org/abs/2306.11251">Eliminating Lipschitz Singularities</a> </li>
  <li><strong>Generative Adversarial Network:</strong> <a href="https://arxiv.org/abs/1906.08090v1">Latently Invertible Autoencoder (LIA)</a>, <a href="https://genforce.github.io/idinvert/">IDInvert</a>, <a href="https://zhujiapeng.github.io/LowRankGAN/">LowRankGAN</a>,  <a href="https://zhujiapeng.github.io/resefa/">ReSeFa</a>,  <a href="https://zhujiapeng.github.io/linkgan/">LinkGAN</a>  </li>
</ul>
  <h2><font size="4"> Multi-Modal Learning</font></h2>
  <ul>
  <li><strong>Image:</strong> <a href="https://github.com/JacobYuan7/RLIPv2">RLIP</a>, <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xie_RA-CLIP_Retrieval_Augmented_Contrastive_Language-Image_Pre-Training_CVPR_2023_paper.html">RA-CLIP</a>, <a href="https://proceedings.mlr.press/v202/zhao23l.html">RLEG</a> </li>
  <li><strong>Video:</strong> <a href="https://arxiv.org/abs/2307.02869">MomentDiff</a> </li>
   <li><strong>Unified framework:</strong>  <a href="https://arxiv.org/abs/2303.06911">ViM</a>,  <a href="https://arxiv.org/abs/2303.00690">U-Tuning</a> </li>
  </ul>
  <h2><font size="4"> Deep-Learning Theory</font></h2>
  <ul>
  <li><strong>Deep neural networks:</strong>  <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/d5cd70b708f726737e2ebace18c3f71b-Abstract-Conference.html">Rank Diminishing in DNNs</a> (cowork with <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/jordan.html">Michael Jordan</a>),  <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Feng_Neural_Dependencies_Emerging_From_Learning_Massive_Categories_CVPR_2023_paper.html">Neural Dependencies</a> (cowork with <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/jordan.html">Michael Jordan</a>) </li>
  <li><strong>Generative models:</strong>  <a href="https://proceedings.mlr.press/v139/feng21g.html">Noise injection in GANs</a>, <a href="https://proceedings.mlr.press/v139/feng21c.html">Uncertainty Principles of Encoding GANs</a> </li>
  
  </ul>
<p></p>

<h2><font size="5">Projects on AIGC </font></h2>
<ul>
<li><strong>Text-to-image:</strong> <a href="https://wanxiang.aliyun.com"> <strong> Alibaba Tongyi Wanxiang (通义万相)</strong> </a>  </li>
<li><strong>Text-to-video:</strong> <a href="https://huggingface.co/damo-vilab/MS-Vid2Vid-XL"> <strong> Video generation</strong></a>, <a href="https://huggingface.co/damo-vilab"> Open-source models on Hugging Face</a>, <a href="https://modelscope.cn/models/damo/Image-to-Video/summary"> Open-source models on ModelScope</a> </li>
</ul>

<hr>

<h2><font size="5">My Book</font></h2>
  
<table width="80%" border="0" bordercolor="#FFFFFF" id="table3">
<tr>
<td width="345" style="padding: 0">
<img border="0" src="my-book.png" width="339.6" height="505.5"></td>
<td width="941" style="padding: 0" height="0">
  <ul>
   <li><strong>Section 1:</strong> <font color="#ACABAB">Preface</font> </li> <pre></pre>
   <li><strong>Section 2:</strong> Curse of Dimensionality</li> <pre></pre>
   <li><strong>Section 3:</strong> Deep Neural Network (in writing)</li> <pre></pre>
   <li><strong>Section 4:</strong> <font color="#ACABAB">Autoencoder</font></li> <pre></pre>
   <li><strong>Section 5:</strong> <font color="#ACABAB">Generative Adversarial Network</font></li> <pre></pre>
   <li><strong>Section 6:</strong> <font color="#ACABAB">Normalizing Flow</font></li> <pre></pre>
   <li><strong>Section 7:</strong> <font color="#ACABAB">Diffusion Model</font></li> <pre></pre>
   <li><strong>Section 8:</strong> <font color="#ACABAB">Autoregressive Model</font></li> <pre></pre>
    <li><strong>Section 9:</strong> <font color="#ACABAB">Reinforcement Learning</font></li> <pre></pre>
   <li><strong>Section 10:</strong> <font color="#ACABAB">Perspective on AI</font></li> 
 </ul>
</td>
</tr>
</table>
  

</body>

</html>
